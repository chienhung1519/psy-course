{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_event(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    rtn = {\"aid\": [], \"pid\": [], \"prefix\": [], \"input_text\": [], \"target_text\": []}\n",
    "\n",
    "    prev_sentence = None\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        # Target text\n",
    "        target_text = []\n",
    "        if not pd.isna(row.Remission) or not pd.isna(row.Response):\n",
    "            target_text.append(\"Remission\")\n",
    "        if not pd.isna(row.Acute):\n",
    "            target_text.append(\"Acute\")\n",
    "        if not pd.isna(row.DayCare):\n",
    "            target_text.append(\"DayCare\")\n",
    "        if not pd.isna(row.Episode):\n",
    "            target_text.append(\"Episode\")\n",
    "        if len(target_text) == 0:\n",
    "            target_text = \"None\"\n",
    "        else:\n",
    "            target_text = \", \".join(target_text)\n",
    "        # Merge to the previous example if the current sentence is the same as previous one\n",
    "        if prev_sentence == row.Sentence:\n",
    "            # Continue if no events and duplicated sentences\n",
    "            if target_text == \"None\":\n",
    "                continue\n",
    "            else:\n",
    "                if rtn[\"target_text\"][-1] == \"None\":\n",
    "                    rtn[\"target_text\"][-1] = target_text\n",
    "                elif target_text not in rtn[\"target_text\"][-1]:\n",
    "                    rtn[\"target_text\"][-1] = f\"{rtn['target_text'][-1]}, {target_text}\"\n",
    "        else:\n",
    "            rtn[\"aid\"].append(row.AID)\n",
    "            rtn[\"pid\"].append(row.PID)\n",
    "            rtn[\"prefix\"].append(\"event detection\")\n",
    "            rtn[\"input_text\"].append(f\"{row.Sentence} order: {row.Order}. options: Remission, Acute, DayCare, Episode.\")\n",
    "            rtn[\"target_text\"].append(target_text)\n",
    "        # Store sentence\n",
    "        prev_sentence = row.Sentence\n",
    "\n",
    "    return pd.DataFrame(rtn)\n",
    "\n",
    "\n",
    "def process_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    rtn = {\"aid\": [], \"pid\": [], \"prefix\": [], \"input_text\": [], \"target_text\": []}\n",
    "\n",
    "    duration_head = None\n",
    "    prev_sentence = None\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        # Target text is None when TimeInfo column is NaN\n",
    "        if pd.isna(row.TimeInfo):\n",
    "            target_text = \"None\"\n",
    "        else:\n",
    "            # Target text is composed by two lines when duration is not NaN\n",
    "            if not pd.isna(row.Duration):\n",
    "                # Duration head\n",
    "                if duration_head is None:\n",
    "                    duration_head = row.Time_YMD\n",
    "                    continue\n",
    "                else:\n",
    "                    target_text = f\"duration: {duration_head} to {row.Time_YMD}\"\n",
    "                    duration_head = None\n",
    "            else:\n",
    "                target_text = []\n",
    "                if not pd.isna(row.Time_YMD):\n",
    "                    target_text.append(f\"time: {row.Time_YMD}.\")\n",
    "                if not pd.isna(row.Vague):\n",
    "                    target_text.append(f\"vague: {row.Vague}.\")\n",
    "                if not pd.isna(row.Age):\n",
    "                    target_text.append(f\"age: {row.Age}.\")\n",
    "                if not pd.isna(row.Ago_YMD):\n",
    "                    target_text.append(f\"ago: {row.Ago_YMD}.\")\n",
    "                assert len(target_text) != 0, row.AID\n",
    "                target_text = \" \".join(target_text)\n",
    "\n",
    "        # Merge to the previous example if the current sentence is the same as previous one\n",
    "        if row.Sentence == prev_sentence:\n",
    "            # Continue if no events and duplicated sentences\n",
    "            if target_text == \"None\":\n",
    "                continue\n",
    "            else:\n",
    "                if rtn[\"target_text\"][-1] == \"None\":\n",
    "                    rtn[\"target_text\"][-1] = target_text\n",
    "                else:\n",
    "                    rtn[\"target_text\"][-1] = f\"{rtn['target_text'][-1]} {target_text}\"\n",
    "        else:\n",
    "            rtn[\"aid\"].append(row.AID)\n",
    "            rtn[\"pid\"].append(row.PID)\n",
    "            rtn[\"prefix\"].append(\"time extraction\")\n",
    "            rtn[\"input_text\"].append(f\"{row.Sentence} admission date: {row.Admissindate}. options: time, vague, age, ago.\")\n",
    "            rtn[\"target_text\"].append(target_text)\n",
    "        # Store sentence\n",
    "        prev_sentence = row.Sentence\n",
    "\n",
    "    return pd.DataFrame(rtn)\n",
    "\n",
    "\n",
    "def process_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    rtn = pd.DataFrame()\n",
    "    rtn = pd.concat([rtn, process_event(df)], ignore_index=False)\n",
    "    rtn = pd.concat([rtn, process_time(df)], ignore_index=False)\n",
    "\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "EXCLUDE_SHEETS = [\"500篇ID說明\", \"500篇ID處理說明\", \"工作表1\"]\n",
    "COLUMNS = [\n",
    "    \"AID\", \"PID\", \"Admissindate\", \"Sentence\",\n",
    "    \"Duration\", \"Time_YMD\", \"Vague\", \"Age\", \"Ago_YMD\", \"TimeInfo\",\n",
    "    \"Remission\", \"Response\", \"緩解時間\", \"Acute\", \"急性住院時間\", \"DayCare\", \"慢性住院時間\", \"Episode\", \"Episode時間\"\n",
    "]\n",
    "\n",
    "data_dir = \"./data/raw/\"\n",
    "data_dir_path = Path(data_dir)\n",
    "processed = pd.DataFrame()\n",
    "for file in data_dir_path.iterdir():\n",
    "    # Load data\n",
    "    data = pd.read_excel(file, sheet_name=None, engine='openpyxl', dtype=str)\n",
    "    # Access sheet name\n",
    "    sheet = pd.ExcelFile(file, engine='openpyxl')\n",
    "    sheet = [s for s in sheet.sheet_names if s not in EXCLUDE_SHEETS]\n",
    "    assert len(sheet) == 1\n",
    "    sheet_name = sheet[0]\n",
    "    # Select sheet and set columns\n",
    "    df = data.get(sheet_name)\n",
    "    df = df[COLUMNS]\n",
    "    df.Admissindate = pd.to_datetime(df.Admissindate, format=\"%Y-%m-%d\")\n",
    "    # Process\n",
    "    processed = pd.concat([processed, process_data(df)], ignore_index=False)\n",
    "\n",
    "# Save\n",
    "processed_file = \"./data/processed/data.xlsx\"\n",
    "processed.to_excel(processed_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train data: 19049\n",
      "Number of validation data: 2721\n",
      "Number of test data: 5443\n",
      "Number of total data: 27213\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel(\"./data/processed/data.xlsx\", engine='openpyxl', dtype=str)\n",
    "train, validate, test = np.split(data.sample(frac=1, random_state=42), [int(.7*len(data)), int(.8*len(data))])\n",
    "print(f\"Number of train data: {len(train)}\")\n",
    "print(f\"Number of validation data: {len(validate)}\")\n",
    "print(f\"Number of test data: {len(test)}\")\n",
    "print(f\"Number of total data: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Datasets\n",
    "\n",
    "* all in one merged by prefix ex. prefix=acute, prefix=vague...\n",
    "* Cannot identify which time does event belongs to => the same sentence may have None and Not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "EVENT_NAME = [\"Remission\", \"Acute\", \"DayCare\", \"Episode\"]\n",
    "\n",
    "\n",
    "def process_event(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    rtn = {\"aid\": [], \"pid\": [], \"prefix\": [], \"input_text\": [], \"target_text\": []}\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        for e_name in EVENT_NAME:\n",
    "            if e_name == \"Remission\" and not pd.isna(row.Response):\n",
    "                target_text == e_name\n",
    "            else:\n",
    "                target_text = \"None\" if pd.isna(getattr(row, e_name)) else e_name\n",
    "            rtn[\"aid\"].append(row.AID)\n",
    "            rtn[\"pid\"].append(row.PID)\n",
    "            rtn[\"prefix\"].append(e_name.lower())\n",
    "            rtn[\"input_text\"].append(row.Sentence)\n",
    "            rtn[\"target_text\"].append(target_text)\n",
    "\n",
    "    df = pd.DataFrame(rtn)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "TIME_NAME = [\"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]\n",
    "\n",
    "def get_time_target_text(prev_info: Dict, t_name:str) -> str:\n",
    "    # The target text is None when there is no values\n",
    "    if len(prev_info[\"values\"]) == 0:\n",
    "        target_text = \"None\"\n",
    "    else:\n",
    "        # Pair the duration values\n",
    "        if t_name == \"Duration\":\n",
    "            target_text = []\n",
    "            assert len(prev_info[\"values\"]) % 2 == 0, prev_info\n",
    "            for i in range(0, len(prev_info[\"values\"]), 2):\n",
    "                target_text.append(f\"{prev_info['values'][i]} to {prev_info['values'][i+1]}\")\n",
    "            target_text = \". \".join(target_text) + \".\"\n",
    "        # Merge the collected values\n",
    "        else:\n",
    "            target_text = \". \".join(prev_info[\"values\"]) + \".\"\n",
    "    return target_text\n",
    "\n",
    "def process_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    rtn = {\"aid\": [], \"pid\": [], \"prefix\": [], \"input_text\": [], \"target_text\": []}\n",
    "\n",
    "    prev_info = {\"aid\": None, \"pid\": None, \"sentence\": None, \"values\": []}\n",
    "\n",
    "    for t_name in TIME_NAME:\n",
    "        for row in df.itertuples():\n",
    "            # Merge data with the same sentence\n",
    "            if row.Sentence == prev_info[\"sentence\"]:\n",
    "                if not pd.isna(getattr(row, t_name)) and not pd.isna(row.TimeInfo):\n",
    "                    if t_name == \"Duration\":\n",
    "                        if not pd.isna(row.Time):\n",
    "                            prev_info[\"values\"].append(row.Time)\n",
    "                        elif not pd.isna(row.Age):\n",
    "                            prev_info[\"values\"].append(row.Age)\n",
    "                        elif not pd.isna(row.Vague):\n",
    "                            prev_info[\"values\"].append(row.Vague)\n",
    "                        elif not pd.isna(row.Ago):\n",
    "                            prev_info[\"values\"].append(row.Ago)\n",
    "                    else:\n",
    "                        prev_info[\"values\"].append(getattr(row, t_name))\n",
    "            # When encounter different sentence, store data and reset information collection\n",
    "            else:\n",
    "                if prev_info[\"aid\"] is not None:\n",
    "                    # Store data instance\n",
    "                    rtn[\"aid\"].append(prev_info[\"aid\"])\n",
    "                    rtn[\"pid\"].append(prev_info[\"pid\"])\n",
    "                    rtn[\"prefix\"].append(t_name.lower())\n",
    "                    rtn[\"input_text\"].append(prev_info[\"sentence\"])\n",
    "                    rtn[\"target_text\"].append(get_time_target_text(prev_info, t_name))\n",
    "\n",
    "                # Reset information collection\n",
    "                if not pd.isna(getattr(row, t_name)) and not pd.isna(row.TimeInfo):\n",
    "                    if t_name == \"Duration\":\n",
    "                        if not pd.isna(row.Time):\n",
    "                            value = [row.Time]\n",
    "                        elif not pd.isna(row.Age):\n",
    "                            value = [row.Age]\n",
    "                        elif not pd.isna(row.Vague):\n",
    "                            value = [row.Vague]\n",
    "                        elif not pd.isna(row.Ago):\n",
    "                            value = [row.Ago]\n",
    "                        else:\n",
    "                            print(f\"{row.AID} {row.PID}\")\n",
    "                    else:\n",
    "                        value = [getattr(row, t_name)]\n",
    "                else:\n",
    "                    value = []\n",
    "                prev_info[\"aid\"] = row.AID\n",
    "                prev_info[\"pid\"] = row.PID\n",
    "                prev_info[\"sentence\"] = row.Sentence\n",
    "                prev_info[\"values\"] = value\n",
    "\n",
    "        # Store data instance\n",
    "        rtn[\"aid\"].append(prev_info[\"aid\"])\n",
    "        rtn[\"pid\"].append(prev_info[\"pid\"])\n",
    "        rtn[\"prefix\"].append(t_name.lower())\n",
    "        rtn[\"input_text\"].append(prev_info[\"sentence\"])\n",
    "        rtn[\"target_text\"].append(get_time_target_text(prev_info, t_name))\n",
    "\n",
    "    return pd.DataFrame(rtn)\n",
    "\n",
    "\n",
    "def process_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    rtn = pd.DataFrame()\n",
    "    rtn = pd.concat([rtn, process_event(df)], ignore_index=False)\n",
    "    rtn = pd.concat([rtn, process_time(df)], ignore_index=False)\n",
    "\n",
    "    return rtn\n",
    "\n",
    "data = pd.read_excel(\"./data/raw/data.xlsx\", sheet_name=None, engine='openpyxl', dtype=str)\n",
    "sheet = pd.ExcelFile(\"./data/raw/data.xlsx\", engine='openpyxl')\n",
    "processed = pd.DataFrame()\n",
    "for s_name in sheet.sheet_names:\n",
    "    df = data.get(s_name)\n",
    "    df.Admissindate = pd.to_datetime(df.Admissindate, format=\"%Y-%m-%d\")\n",
    "    processed = pd.concat([processed, process_data(df)], ignore_index=False)\n",
    "\n",
    "# Save\n",
    "processed.to_excel(\"./data/processed/data_binary.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "data = pd.read_excel(\"./data/processed/data_binary.xlsx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\", cache_dir=\"/nfs/nas-7.1/chchen/cache/huggingface/\")\n",
    "input_len = [len(tokenizer(text)[\"input_ids\"]) for text in data.input_text]\n",
    "target_len = [len(tokenizer(text)[\"input_ids\"]) for text in data.target_text]\n",
    "print(max(input_len), max(target_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/home/chchen/.local/lib/python3.8/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "EXCLUDE_SHEETS = [\"500篇ID說明\", \"500篇ID處理說明\", \"工作表1\"]\n",
    "COLUMNS = [\n",
    "    \"AID\", \"PID\", \"Admissindate\", \"Sentence\",\n",
    "    \"Duration\", \"Time_YMD\", \"Vague\", \"Age\", \"Ago_YMD\", \"TimeInfo\",\n",
    "    \"Remission\", \"Response\", \"緩解時間\", \"Acute\", \"急性住院時間\", \"DayCare\", \"慢性住院時間\", \"Episode\", \"Episode時間\"\n",
    "]\n",
    "\n",
    "data_dir = \"./data/raw/\"\n",
    "data_dir_path = Path(data_dir)\n",
    "processed = pd.DataFrame()\n",
    "for file in data_dir_path.iterdir():\n",
    "    # Load data\n",
    "    data = pd.read_excel(file, sheet_name=None, engine='openpyxl', dtype=str)\n",
    "    # Access sheet name\n",
    "    sheet = pd.ExcelFile(file, engine='openpyxl')\n",
    "    sheet = [s for s in sheet.sheet_names if s not in EXCLUDE_SHEETS]\n",
    "    assert len(sheet) == 1\n",
    "    sheet_name = sheet[0]\n",
    "    # Select sheet and set columns\n",
    "    df = data.get(sheet_name)\n",
    "    df = df[COLUMNS]\n",
    "    df.Admissindate = pd.to_datetime(df.Admissindate, format=\"%Y-%m-%d\")\n",
    "    # Process\n",
    "    processed = pd.concat([processed, process_data(df)], ignore_index=False)\n",
    "\n",
    "# Save\n",
    "processed_file = \"./data/processed/data_binary.xlsx\"\n",
    "processed.to_excel(processed_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_order(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Indicate the order of sentence.\"\"\"\n",
    "\n",
    "    aid = df[\"AID\"].values.tolist()\n",
    "    \n",
    "    order = []\n",
    "    cur_aid = None\n",
    "    cur_order = 0\n",
    "    for idx in aid:\n",
    "        if cur_aid != idx:\n",
    "            cur_aid = idx\n",
    "            cur_order = 0\n",
    "        order.append(cur_order)\n",
    "        cur_order += 1\n",
    "\n",
    "    df[\"Order\"] = order\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use rule to inference event time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_file = \"./outputs/results.xlsx\"\n",
    "df = pd.read_excel(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90       220\n",
      "           1       0.79      0.39      0.52        28\n",
      "           2       0.52      0.36      0.42       192\n",
      "           3       0.68      0.53      0.60       122\n",
      "           4       0.92      0.95      0.93      2144\n",
      "\n",
      "   micro avg       0.89      0.88      0.88      2706\n",
      "   macro avg       0.76      0.63      0.68      2706\n",
      "weighted avg       0.88      0.88      0.87      2706\n",
      " samples avg       0.89      0.88      0.88      2706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Event\n",
    "df_event = df[df[\"prefix\"] == \"event detection\"]\n",
    "y_true = []\n",
    "y_pred = []\n",
    "labels = {\"Acute\": 0, \"DayCare\":1 , \"Episode\": 2, \"Remission\": 3, \"None\": 4}\n",
    "for y in df_event.target_text.values.tolist():\n",
    "    vector = [0 for _ in range(len(labels))]\n",
    "    for y_i in y.split(\",\"):\n",
    "        y_i = y_i.strip()\n",
    "        vector[labels[y_i]] = 1\n",
    "    y_true.append(vector)\n",
    "for y in df_event.pred_text.values.tolist():\n",
    "    vector = [0 for _ in range(len(labels))]\n",
    "    for y_i in y.split(\",\"):\n",
    "        y_i = y_i.strip()\n",
    "        vector[labels[y_i]] = 1\n",
    "    y_pred.append(vector)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880246020260492\n"
     ]
    }
   ],
   "source": [
    "df_time = df[df[\"prefix\"] == \"time extraction\"]\n",
    "correct = sum([target == pred for target, pred in zip(df_time.target_text.values, df_time.pred_text.values)])\n",
    "correct_rate = correct / len(df_time)\n",
    "print(correct_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7480490523968785\n"
     ]
    }
   ],
   "source": [
    "df_time = df[df[\"prefix\"] == \"time extraction\"]\n",
    "correct = sum([target == pred for target, pred in zip(df_time.target_text.values, df_time.pred_text.values)if target != \"None\"])\n",
    "total = sum([1 for target in df_time.target_text.values if target != \"None\"])\n",
    "correct_rate = correct / total\n",
    "print(correct_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_event_metric(labels: List, preds: List) -> str:\n",
    "    return classification_report(labels, preds, digits=4)\n",
    "\n",
    "def compute_time_metric(labels: List, preds: List) -> Dict:\n",
    "    df = pd.DataFrame({\"labels\": labels, \"preds\": preds})\n",
    "    df_right = df[df.labels==df.preds]\n",
    "    df_wrong = df[df.labels!=df.preds]\n",
    "    return {\n",
    "        \"tp\": len(df_right[(df_right.labels!=\"None\") & (df_right.preds!=\"None\")]),\n",
    "        \"tn\": len(df_right[(df_right.labels==\"None\") & (df_right.preds==\"None\")]),\n",
    "        \"fp\": len(df_wrong[(df_wrong.labels==\"None\") & (df_wrong.preds!=\"None\")]),\n",
    "        \"fn\": len(df_wrong[(df_wrong.labels!=\"None\") & (df_wrong.preds==\"None\")]),\n",
    "        \"ff\": len(df_wrong[(df_wrong.labels!=\"None\") & (df_wrong.preds!=\"None\")])\n",
    "    }\n",
    "\n",
    "def load_result(path: str) -> pd.DataFrame:\n",
    "    return pd.read_excel(path)\n",
    "\n",
    "\n",
    "result_path = \"./outputs/mt5-binary/results.xlsx\"\n",
    "output_dir = \"./outputs/mt5-binary/\"\n",
    "data = load_result(result_path)\n",
    "\n",
    "# Event\n",
    "for event in [\"remission\", \"acute\", \"daycare\", \"episode\"]:\n",
    "    df = data[data.prefix==event]\n",
    "    metric = compute_event_metric(\n",
    "        df.target_text.values.tolist(), df.pred_text.values.tolist()\n",
    "    )\n",
    "    output_path = Path(output_dir) / f\"cr_{event}.txt\"\n",
    "    output_path.write_text(metric)\n",
    "\n",
    "# Time\n",
    "for time in [\"duration\", \"time\", \"vague\", \"age\", \"ago\", \"persistence\"]:\n",
    "    df = data[data.prefix==time]\n",
    "    metric = compute_time_metric(\n",
    "        df.target_text.values.tolist(), df.pred_text.values.tolist()\n",
    "    )\n",
    "    output_path = Path(output_dir) / f\"cm_{time}.txt\"\n",
    "    output_path.write_text(json.dumps(metric, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Event and Time Concurrently"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_chinese(text):\n",
    "    pattern = re.compile(\"[\\u4e00-\\u9fa5]+\")\n",
    "    return pattern.findall(str(text))\n",
    "\n",
    "data_path = \"./data/raw/data.xlsx\"\n",
    "data = pd.read_excel(data_path)\n",
    "chinese_segments = []\n",
    "for row in data.itertuples():\n",
    "    for col in [\"Sentence\", \"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]:\n",
    "        if pd.isna(getattr(row, col)):\n",
    "            continue\n",
    "        segments = extract_chinese(getattr(row, col))\n",
    "        if len(segments) != 0:\n",
    "            chinese_segments.extend(segments)\n",
    "chieses_segments = list(set(chinese_segments))\n",
    "pd.DataFrame({\"text\": chieses_segments}).to_excel(\"./data/processed/chinese_segments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chieses_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chinese_segments_translated = pd.read_excel(\"./data/processed/chinese_segments_translated.xlsx\")\n",
    "chinese_to_english = {row.Chinese: row.English for row in chinese_segments_translated.itertuples()}\n",
    "\n",
    "def replace_chinese_with_english(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    segments = extract_chinese(text)\n",
    "    if len(segments) == 0:\n",
    "        return text\n",
    "    segments = sorted(segments, key=len, reverse=True)\n",
    "    for seg in segments:\n",
    "        text = text.replace(seg, chinese_to_english[seg])\n",
    "    return text\n",
    "\n",
    "data_path = \"./data/raw/data.xlsx\"\n",
    "data = pd.read_excel(data_path)\n",
    "for col in [\"Sentence\", \"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]:\n",
    "    data[col] = data[col].apply(replace_chinese_with_english)\n",
    "\n",
    "data.to_excel(\"./data/raw/data_translated.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_chinese(text):\n",
    "    pattern = re.compile(\"[\\u4e00-\\u9fa5]+\")\n",
    "    return pattern.findall(str(text))\n",
    "\n",
    "data_path = \"./data/raw/data_translated.xlsx\"\n",
    "data = pd.read_excel(data_path)\n",
    "chinese_segments = []\n",
    "for row in data.itertuples():\n",
    "    for col in [\"Sentence\", \"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]:\n",
    "        if pd.isna(getattr(row, col)):\n",
    "            continue\n",
    "        segments = extract_chinese(getattr(row, col))\n",
    "        if len(segments) != 0:\n",
    "            chinese_segments.extend(segments)\n",
    "chinese_segments = list(set(chinese_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TIME_NAME = [\"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]\n",
    "EVENT_NAME = [\"Remission\", \"Response\", \"Acute\", \"DayCare\", \"Episode\"]\n",
    "\n",
    "data = pd.read_excel(\"./data/raw/data.xlsx\", sheet_name=None, engine='openpyxl', dtype=str)\n",
    "sheet = pd.ExcelFile(\"./data/raw/data.xlsx\", engine='openpyxl')\n",
    "concat_df = pd.concat([data.get(s_name) for s_name in sheet.sheet_names])\n",
    "for t_name in TIME_NAME:\n",
    "    concat_df[t_name] = concat_df[t_name].fillna(\"\")\n",
    "    concat_df[t_name] = concat_df[t_name].astype(str)\n",
    "for e_name in EVENT_NAME+[\"TimeInfo\"]:\n",
    "    concat_df[e_name] = concat_df[e_name].fillna(0)\n",
    "    concat_df[e_name] = concat_df[e_name].astype(int)\n",
    "\n",
    "concat_df = concat_df[concat_df.Vague != \"Fact\"]\n",
    "\n",
    "concat_df = concat_df.loc[:, [\"AID\", \"PID\", \"Sentence\", \"TimeInfo\"]+TIME_NAME+EVENT_NAME]\n",
    "\n",
    "def clear_no_time(row):\n",
    "    if row.TimeInfo == 0:\n",
    "        for t_name in TIME_NAME:\n",
    "            setattr(row, t_name, \"\")\n",
    "    return row\n",
    "\n",
    "concat_df = concat_df.apply(clear_no_time, axis=1)\n",
    "\n",
    "def extract_duration(row):\n",
    "    if row.Duration != \"\":\n",
    "        for t_name in TIME_NAME[1:]:\n",
    "            if getattr(row, t_name) != \"\":\n",
    "                row.Duration = getattr(row, t_name)\n",
    "                setattr(row, t_name, \"\")\n",
    "                break\n",
    "    return row\n",
    "\n",
    "concat_df = concat_df.apply(extract_duration, axis=1)\n",
    "\n",
    "# https://stackoverflow.com/questions/33279940/how-to-combine-multiple-rows-of-strings-into-one-using-pandas\n",
    "concat_df = concat_df.groupby([\"AID\", \"PID\", \"Sentence\", \"TimeInfo\"]).agg({\n",
    "    \"Duration\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Time\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Vague\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Age\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Ago\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Persistence\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Remission\": lambda x: sum(x),\n",
    "    \"Response\": lambda x: sum(x),\n",
    "    \"Acute\": lambda x: sum(x),\n",
    "    \"DayCare\": lambda x: sum(x),\n",
    "    \"Episode\": lambda x: sum(x),\n",
    "})\n",
    "\n",
    "# Convert Index to Column\n",
    "concat_df = concat_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_chinese(text):\n",
    "    pattern = re.compile(\"[\\u4e00-\\u9fa5]+\")\n",
    "    return pattern.findall(str(text))\n",
    "\n",
    "chinese_segments = []\n",
    "for row in concat_df.itertuples():\n",
    "    for col in [\"Sentence\", \"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]:\n",
    "        if pd.isna(getattr(row, col)):\n",
    "            continue\n",
    "        segments = extract_chinese(getattr(row, col))\n",
    "        if len(segments) != 0:\n",
    "            chinese_segments.extend(segments)\n",
    "chieses_segments = list(set(chinese_segments))\n",
    "pd.DataFrame({\"text\": chieses_segments}).to_excel(\"./data/processed/chinese_segments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chinese_segments_translated = pd.read_excel(\"./data/processed/chinese_segments_translated.xlsx\")\n",
    "chinese_to_english = {row.Chinese: row.English for row in chinese_segments_translated.itertuples()}\n",
    "\n",
    "def replace_chinese_with_english(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    segments = extract_chinese(text)\n",
    "    if len(segments) == 0:\n",
    "        return text\n",
    "    segments = sorted(segments, key=len, reverse=True)\n",
    "    for seg in segments:\n",
    "        text = text.replace(seg, chinese_to_english[seg])\n",
    "    return text\n",
    "\n",
    "for col in [\"Sentence\", \"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]:\n",
    "    concat_df[col] = concat_df[col].apply(replace_chinese_with_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_duration(x):\n",
    "    if x == \"\":\n",
    "        return x\n",
    "    rtn = []\n",
    "    durations = x.split(\", \")\n",
    "    for i in range(0, len(durations), 2):\n",
    "        rtn.append(f\"{durations[i]} to {durations[i+1]}\")\n",
    "    return \", \".join(rtn)\n",
    "\n",
    "concat_df[\"Duration\"] = concat_df[\"Duration\"].apply(concat_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_NAME = [\"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]\n",
    "EVENT_NAME = [\"Remission\", \"Response\", \"Acute\", \"DayCare\", \"Episode\"]\n",
    "\n",
    "parsed_data = []\n",
    "for row in concat_df.itertuples():\n",
    "    time_list = []\n",
    "    for t_name in TIME_NAME:\n",
    "        if getattr(row, t_name) != \"\":\n",
    "            time_list.append(f\"{t_name}: {getattr(row, t_name)}\")\n",
    "    time_text = \"None\" if len(time_list) == 0 else \". \".join(time_list)\n",
    "    parsed_data.append([row.AID, row.PID, \"time extraction\", row.Sentence, time_text])\n",
    "    \n",
    "    event_list = []\n",
    "    for e_name in EVENT_NAME:\n",
    "        if getattr(row, e_name) > 0:\n",
    "            if e_name == \"Response\":\n",
    "                if \"Remission\" not in event_list:\n",
    "                    event_list.append(\"Remission\")\n",
    "            else:\n",
    "                event_list.append(e_name)\n",
    "    event_text = \"None\" if len(event_list) == 0 else \", \".join(event_list)\n",
    "    parsed_data.append([row.AID, row.PID, \"event detection\", row.Sentence, event_text])\n",
    "\n",
    "pd.DataFrame(\n",
    "    parsed_data, \n",
    "    columns=[\"aid\", \"pid\", \"prefix\", \"input_text\", \"target_text\"]\n",
    ").to_excel(\"./data/processed/data_translated_parsed.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 78\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "data = pd.read_excel(\"./data/processed/data_translated_parsed.xlsx\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", cache_dir=\"/nfs/nas-7.1/chchen/cache/huggingface/\")\n",
    "input_len = [len(tokenizer(text)[\"input_ids\"]) for text in data.input_text]\n",
    "target_len = [len(tokenizer(text)[\"input_ids\"]) for text in data.target_text]\n",
    "print(max(input_len), max(target_len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"./outputs/t5-translated/outputs.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.88       234\n",
      "           1       0.58      0.41      0.48        17\n",
      "           2       0.80      0.84      0.82      1010\n",
      "           3       0.72      0.71      0.71       184\n",
      "           4       0.86      0.82      0.84      1223\n",
      "\n",
      "   micro avg       0.83      0.82      0.82      2668\n",
      "   macro avg       0.77      0.73      0.75      2668\n",
      "weighted avg       0.83      0.82      0.82      2668\n",
      " samples avg       0.83      0.83      0.82      2668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df_event = df[df[\"prefix\"] == \"event detection\"]\n",
    "y_true = []\n",
    "y_pred = []\n",
    "labels = {\"Acute\": 0, \"DayCare\":1 , \"Episode\": 2, \"Remission\": 3, \"None\": 4}\n",
    "for y in df_event.target_text.values.tolist():\n",
    "    vector = [0 for _ in range(len(labels))]\n",
    "    for y_i in y.split(\",\"):\n",
    "        y_i = y_i.strip()\n",
    "        if y_i == \"Response\":\n",
    "            y_i = \"Remission\"\n",
    "        vector[labels[y_i]] = 1\n",
    "    y_true.append(vector)\n",
    "for y in df_event.pred_text.values.tolist():\n",
    "    vector = [0 for _ in range(len(labels))]\n",
    "    for y_i in y.split(\",\"):\n",
    "        y_i = y_i.strip()\n",
    "        # if y_i not in labels:\n",
    "        #     y_i = \"None\"\n",
    "        if y_i == \"Response\":\n",
    "            y_i = \"Remission\"\n",
    "        vector[labels[y_i]] = 1\n",
    "    y_pred.append(vector)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8875286916602907\n"
     ]
    }
   ],
   "source": [
    "df_time = df[df[\"prefix\"] == \"time extraction\"]\n",
    "correct = sum([target == pred for target, pred in zip(df_time.target_text.values, df_time.pred_text.values)])\n",
    "correct_rate = correct / len(df_time)\n",
    "print(correct_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 35/57=0.614\n",
      "Time: 470/548=0.8577\n",
      "Vague: 45/145=0.3103\n",
      "Age: 68/86=0.7907\n",
      "Ago: 148/216=0.6852\n",
      "Persistence: 4/43=0.093\n"
     ]
    }
   ],
   "source": [
    "TIME_NAME = [\"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]\n",
    "\n",
    "df_time = df[df[\"prefix\"] == \"time extraction\"]\n",
    "\n",
    "for t_name in TIME_NAME:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for row in df_time.itertuples():\n",
    "        target_index = row.target_text.find(t_name)\n",
    "        if target_index == -1:\n",
    "            target_text = \"None\"\n",
    "        else:\n",
    "            target_end_index = row.target_text.find(\".\", target_index)\n",
    "            target_end_index = len(row.target_text) if target_end_index == -1 else target_end_index\n",
    "            target_text = row.target_text[target_index:target_end_index]\n",
    "        \n",
    "        pred_index = row.pred_text.find(t_name)\n",
    "        if pred_index == -1:\n",
    "            pred_text = \"None\"\n",
    "        else:\n",
    "            pred_end_index = row.pred_text.find(\".\", pred_index)\n",
    "            pred_end_index = len(row.pred_text) if pred_end_index == -1 else pred_end_index\n",
    "            pred_text = row.pred_text[pred_index:pred_end_index]\n",
    "\n",
    "        if target_text == pred_text == \"None\":\n",
    "            continue\n",
    "\n",
    "        if target_text == pred_text:\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    print(f\"{t_name}: {correct}/{total}={round(correct/total, 4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Chinese segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chinese(text):\n",
    "    pattern = re.compile(\"[\\u4e00-\\u9fa5]+\")\n",
    "    return pattern.findall(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data/raw/w6/data_230406.xlsx\")\n",
    "chinese_segments = []\n",
    "for i, row in data.iterrows():\n",
    "    for col in [\"Sentence\", \"Duration\", \"Time_YMD\", \"Vague\", \"Age\", \"Ago_YMD\", \"Persistence\"]:\n",
    "        if pd.isna(row[col]):\n",
    "            continue\n",
    "        segments = extract_chinese(row[col])\n",
    "        if len(segments) > 0:\n",
    "            chinese_segments.extend(segments)\n",
    "chieses_segments = list(set(chinese_segments))\n",
    "pd.DataFrame({\"text\": chieses_segments}).to_excel(\"data/raw/w6/chinese_segments.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using Google translation, replace Chinese with English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_to_english = pd.read_excel(\"data/raw/w6/chinese_to_english.xlsx\")\n",
    "chinese_to_english = {row.Chinese: row.English for row in chinese_to_english.itertuples()}\n",
    "\n",
    "def replace_chinese_with_english(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    segments = extract_chinese(text)\n",
    "    if len(segments) == 0:\n",
    "        return text\n",
    "    # Longest match first\n",
    "    segments = sorted(segments, key=len, reverse=True)\n",
    "    for seg in segments:\n",
    "        text = text.replace(seg, f\" {chinese_to_english[seg]} \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"Sentence\", \"Duration\", \"Time_YMD\", \"Vague\", \"Age\", \"Ago_YMD\", \"Persistence\"]:\n",
    "    data[col] = data[col].apply(replace_chinese_with_english)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sentence id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ids = []\n",
    "cur_sent_id = 0\n",
    "cur_aid = \"\"\n",
    "cur_sentence = \"\"\n",
    "for row in data.itertuples():\n",
    "    if row.AID != cur_aid:\n",
    "        cur_aid = row.AID\n",
    "        cur_sent_id = 0\n",
    "        cur_sentence = \"\"\n",
    "    if row.Sentence != cur_sentence:\n",
    "        cur_sentence = row.Sentence\n",
    "        cur_sent_id += 1\n",
    "        sentence_ids.append(cur_sent_id)\n",
    "    else:\n",
    "        sentence_ids.append(cur_sent_id)\n",
    "data[\"Sentence_id\"] = sentence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"data/raw/w6/data_230406_translated.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data/raw/w6/data_230406_translated.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_NAME = [\"Duration\", \"Time_YMD\", \"Vague\", \"Age\", \"Ago_YMD\", \"Persistence\"]\n",
    "EVENT_NAME = [\"Remission\", \"Acute\", \"DayCare\", \"Episode\"]\n",
    "\n",
    "for t_name in TIME_NAME:\n",
    "    data[t_name] = data[t_name].fillna(\"\")\n",
    "    data[t_name] = data[t_name].astype(str)\n",
    "for e_name in EVENT_NAME+[\"TimeInfo\"]:\n",
    "    data[e_name] = data[e_name].fillna(0)\n",
    "    data[e_name] = data[e_name].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out Vague is Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.Vague != \"Fact\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear out the time expression when TimeInfo is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_no_time(row):\n",
    "    if row.TimeInfo == 0:\n",
    "        for t_name in TIME_NAME:\n",
    "            setattr(row, t_name, \"\")\n",
    "    return row\n",
    "data = data.apply(clear_no_time, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_duration(row):\n",
    "    if row.Duration == \"\":\n",
    "        return row\n",
    "    for t_name in [\"Time_YMD\", \"Vague\", \"Age\", \"Ago_YMD\", \"Persistence\"]:\n",
    "        if getattr(row, t_name) != \"\":\n",
    "            row.Duration = getattr(row, t_name)\n",
    "            setattr(row, t_name, \"\")\n",
    "            break\n",
    "    return row\n",
    "data = data.apply(assign_duration, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the data according to AID and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/33279940/how-to-combine-multiple-rows-of-strings-into-one-using-pandas\n",
    "data = data.groupby([\"AID\", \"PID\", \"Sentence_id\", \"Sentence\", \"TimeInfo\"]).agg({\n",
    "    \"Duration\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Time_YMD\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Vague\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Age\": lambda x: ', '.join([x_i.replace(\".0\", \"\") for x_i in x if x_i != \"\"]),\n",
    "    \"Ago_YMD\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Persistence\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Remission\": lambda x: sum(x),\n",
    "    \"Acute\": lambda x: sum(x),\n",
    "    \"DayCare\": lambda x: sum(x),\n",
    "    \"Episode\": lambda x: sum(x),\n",
    "})\n",
    "\n",
    "# Convert Index to Column\n",
    "data = data.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duration errors\n",
    "data_d = data[data[\"Duration\"] != \"\"]\n",
    "data_d[\"check\"] = data_d[\"Duration\"].apply(lambda x: len(x.split(\",\")) % 2 == 0)\n",
    "data_d[data_d[\"check\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_duration(x):\n",
    "    if x == \"\":\n",
    "        return x\n",
    "    rtn = []\n",
    "    durations = x.split(\", \")\n",
    "    for i in range(0, len(durations), 2):\n",
    "        rtn.append(f\"{durations[i]} to {durations[i+1]}\")\n",
    "    return \", \".join(rtn)\n",
    "data[\"Duration\"] = data[\"Duration\"].apply(concat_duration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove DayCare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"DayCare\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse data to simpletransformers format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = []\n",
    "for row in data.itertuples():\n",
    "    time_list = []\n",
    "    for t_name in TIME_NAME:\n",
    "        if getattr(row, t_name) != \"\":\n",
    "            time_list.append(f\"{t_name}: {getattr(row, t_name)}\")\n",
    "    time_text = \"None\" if len(time_list) == 0 else \". \".join(time_list)\n",
    "    parsed_data.append([row.AID, row.PID, row.Sentence_id, \"time extraction\", row.Sentence, time_text])\n",
    "    \n",
    "    event_list = []\n",
    "    for e_name in EVENT_NAME:\n",
    "        if e_name == \"DayCare\":\n",
    "            continue\n",
    "        if getattr(row, e_name) > 0:\n",
    "            event_list.append(e_name)\n",
    "    event_text = \"None\" if len(event_list) == 0 else \", \".join(event_list)\n",
    "    parsed_data.append([row.AID, row.PID, row.Sentence_id, \"event detection\", row.Sentence, event_text])\n",
    "\n",
    "parsed_data = pd.DataFrame(\n",
    "    parsed_data, \n",
    "    columns=[\"AID\", \"PID\", \"Sentence_id\", \"prefix\", \"input_text\", \"target_text\"]\n",
    ")\n",
    "parsed_data.to_excel(\"./data/processed/data_230406.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset by AID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18240, Eval: 2420, Test: 5248\n"
     ]
    }
   ],
   "source": [
    "SEED = 1309\n",
    "TRAIN_RATIO = 0.7\n",
    "EVAL_RATIO = 0.1\n",
    "aid = data.AID.unique()\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(aid)\n",
    "train_aid, eval_aid, test_aid = np.split(aid, [int(TRAIN_RATIO*len(aid)), int((TRAIN_RATIO+EVAL_RATIO)*len(aid))])\n",
    "train_data = parsed_data[parsed_data.AID.isin(train_aid)]\n",
    "eval_data = parsed_data[parsed_data.AID.isin(eval_aid)]\n",
    "test_data = parsed_data[parsed_data.AID.isin(test_aid)]\n",
    "print(f\"Train: {len(train_data)}, Eval: {len(eval_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_excel(\"./data/processed/data_230406_train.xlsx\", index=False)\n",
    "eval_data.to_excel(\"./data/processed/data_230406_eval.xlsx\", index=False)\n",
    "test_data.to_excel(\"./data/processed/data_230406_test.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"outputs/flan-t5-base/outputs.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9248    0.5146    0.6613       239\n",
      "           1     0.0000    0.0000    0.0000        17\n",
      "           2     0.7987    0.7980    0.7983      1089\n",
      "           3     0.6942    0.6590    0.6761       217\n",
      "           4     0.7725    0.8328    0.8015      1154\n",
      "\n",
      "   micro avg     0.7847    0.7717    0.7782      2716\n",
      "   macro avg     0.6380    0.5609    0.5875      2716\n",
      "weighted avg     0.7853    0.7717    0.7729      2716\n",
      " samples avg     0.7873    0.7795    0.7805      2716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "df_event = df[df[\"prefix\"] == \"event detection\"]\n",
    "y_true, y_pred = [], []\n",
    "labels = {\"Acute\": 0, \"DayCare\":1 , \"Episode\": 2, \"Remission\": 3, \"None\": 4}\n",
    "for y in df_event.target_text.values.tolist():\n",
    "    vector = [0 for _ in range(len(labels))]\n",
    "    for y_i in y.split(\",\"):\n",
    "        y_i = y_i.strip()\n",
    "        vector[labels[y_i]] = 1\n",
    "    y_true.append(vector)\n",
    "for y in df_event.pred_text.values.tolist():\n",
    "    vector = [0 for _ in range(len(labels))]\n",
    "    for y_i in y.split(\",\"):\n",
    "        y_i = y_i.strip()\n",
    "        vector[labels[y_i]] = 1\n",
    "    y_pred.append(vector)\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 18/64=0.2812\n",
      "Time: 423/520=0.8135\n",
      "Vague: 42/157=0.2675\n",
      "Age: 62/102=0.6078\n",
      "Ago: 115/199=0.5779\n",
      "Persistence: 4/39=0.1026\n"
     ]
    }
   ],
   "source": [
    "TIME_NAME = [\"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]\n",
    "\n",
    "df_time = df[df[\"prefix\"] == \"time extraction\"]\n",
    "\n",
    "for t_name in TIME_NAME:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for row in df_time.itertuples():\n",
    "        target_index = row.target_text.find(t_name)\n",
    "        if target_index == -1:\n",
    "            target_text = \"None\"\n",
    "        else:\n",
    "            target_end_index = row.target_text.find(\".\", target_index)\n",
    "            target_end_index = len(row.target_text) if target_end_index == -1 else target_end_index\n",
    "            target_text = row.target_text[target_index:target_end_index]\n",
    "        \n",
    "        pred_index = row.pred_text.find(t_name)\n",
    "        if pred_index == -1:\n",
    "            pred_text = \"None\"\n",
    "        else:\n",
    "            pred_end_index = row.pred_text.find(\".\", pred_index)\n",
    "            pred_end_index = len(row.pred_text) if pred_end_index == -1 else pred_end_index\n",
    "            pred_text = row.pred_text[pred_index:pred_end_index]\n",
    "\n",
    "        if target_text == pred_text == \"None\":\n",
    "            continue\n",
    "\n",
    "        if target_text == pred_text:\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    print(f\"{t_name}: {correct}/{total}={round(correct/total, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Acute     0.9307    0.7866    0.8526       239\n",
      "     Episode     0.8163    0.7998    0.8080      1089\n",
      "   Remission     0.7744    0.5853    0.6667       217\n",
      "        None     0.7713    0.8634    0.8148      1164\n",
      "\n",
      "   micro avg     0.8008    0.8088    0.8048      2709\n",
      "   macro avg     0.8232    0.7588    0.7855      2709\n",
      "weighted avg     0.8037    0.8088    0.8035      2709\n",
      " samples avg     0.8055    0.8100    0.8051      2709\n",
      "\n",
      "Duration: 28/60=0.4667\n",
      "Time: 429/514=0.8346\n",
      "Vague: 60/149=0.4027\n",
      "Age: 79/96=0.8229\n",
      "Ago: 140/201=0.6965\n",
      "Persistence: 4/43=0.093\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_excel(\"outputs/t5-base/outputs.xlsx\")\n",
    "\n",
    "df_event = df[df[\"prefix\"] == \"event detection\"]\n",
    "y_true, y_pred = [], []\n",
    "labels = {\"Acute\": 0, \"Episode\": 1, \"Remission\": 2, \"None\": 3}\n",
    "for y in df_event.target_text.values.tolist():\n",
    "    vector = [0 for _ in range(len(labels))]\n",
    "    for y_i in y.split(\",\"):\n",
    "        y_i = y_i.strip()\n",
    "        vector[labels[y_i]] = 1\n",
    "    y_true.append(vector)\n",
    "for y in df_event.pred_text.values.tolist():\n",
    "    vector = [0 for _ in range(len(labels))]\n",
    "    for y_i in y.split(\",\"):\n",
    "        y_i = y_i.strip()\n",
    "        vector[labels[y_i]] = 1\n",
    "    y_pred.append(vector)\n",
    "print(classification_report(y_true, y_pred, digits=4, target_names=list(labels.keys())))\n",
    "\n",
    "TIME_NAME = [\"Duration\", \"Time\", \"Vague\", \"Age\", \"Ago\", \"Persistence\"]\n",
    "\n",
    "df_time = df[df[\"prefix\"] == \"time extraction\"]\n",
    "\n",
    "for t_name in TIME_NAME:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for row in df_time.itertuples():\n",
    "        target_index = row.target_text.find(t_name)\n",
    "        if target_index == -1:\n",
    "            target_text = \"None\"\n",
    "        else:\n",
    "            target_end_index = row.target_text.find(\".\", target_index)\n",
    "            target_end_index = len(row.target_text) if target_end_index == -1 else target_end_index\n",
    "            target_text = row.target_text[target_index:target_end_index]\n",
    "        \n",
    "        pred_index = row.pred_text.find(t_name)\n",
    "        if pred_index == -1:\n",
    "            pred_text = \"None\"\n",
    "        else:\n",
    "            pred_end_index = row.pred_text.find(\".\", pred_index)\n",
    "            pred_end_index = len(row.pred_text) if pred_end_index == -1 else pred_end_index\n",
    "            pred_text = row.pred_text[pred_index:pred_end_index]\n",
    "\n",
    "        if target_text == pred_text == \"None\":\n",
    "            continue\n",
    "\n",
    "        if target_text == pred_text:\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    print(f\"{t_name}: {correct}/{total}={round(correct/total, 4)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output data with TimeInfo not set\n",
    "* `ni`: with no TimeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel(\"data/raw/w6/data_230406_translated.xlsx\")\n",
    "\n",
    "TIME_NAME = [\"Duration\", \"Time_YMD\", \"Vague\", \"Age\", \"Ago_YMD\", \"Persistence\"]\n",
    "EVENT_NAME = [\"Remission\", \"Acute\", \"DayCare\", \"Episode\"]\n",
    "\n",
    "for t_name in TIME_NAME:\n",
    "    data[t_name] = data[t_name].fillna(\"\")\n",
    "    data[t_name] = data[t_name].astype(str)\n",
    "for e_name in EVENT_NAME+[\"TimeInfo\"]:\n",
    "    data[e_name] = data[e_name].fillna(0)\n",
    "    data[e_name] = data[e_name].astype(int)\n",
    "\n",
    "def assign_duration(row):\n",
    "    if row.Duration == \"\":\n",
    "        return row\n",
    "    for t_name in [\"Time_YMD\", \"Vague\", \"Age\", \"Ago_YMD\", \"Persistence\"]:\n",
    "        if getattr(row, t_name) != \"\":\n",
    "            row.Duration = getattr(row, t_name)\n",
    "            setattr(row, t_name, \"\")\n",
    "            break\n",
    "    return row\n",
    "data = data.apply(assign_duration, axis=1)\n",
    "\n",
    "data = data.groupby([\"AID\", \"PID\", \"Sentence_id\", \"Sentence\", \"TimeInfo\"]).agg({\n",
    "    \"Duration\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Time_YMD\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Vague\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Age\": lambda x: ', '.join([x_i.replace(\".0\", \"\") for x_i in x if x_i != \"\"]),\n",
    "    \"Ago_YMD\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Persistence\": lambda x: ', '.join([x_i for x_i in x if x_i != \"\"]),\n",
    "    \"Remission\": lambda x: sum(x),\n",
    "    \"Acute\": lambda x: sum(x),\n",
    "    \"DayCare\": lambda x: sum(x),\n",
    "    \"Episode\": lambda x: sum(x),\n",
    "})\n",
    "data = data.reset_index()\n",
    "\n",
    "def concat_duration(x):\n",
    "    if x == \"\":\n",
    "        return x\n",
    "    rtn = []\n",
    "    durations = x.split(\", \")\n",
    "    for i in range(0, len(durations), 2):\n",
    "        rtn.append(f\"{durations[i]} to {durations[i+1]}\")\n",
    "    return \", \".join(rtn)\n",
    "data[\"Duration\"] = data[\"Duration\"].apply(concat_duration)\n",
    "\n",
    "data.drop(columns=[\"DayCare\"], inplace=True)\n",
    "\n",
    "data.to_excel(\"data/raw/w6/data_230406_ni.xlsx\", index=False)\n",
    "\n",
    "parsed_data = []\n",
    "for row in data.itertuples():\n",
    "    time_list = []\n",
    "    for t_name in TIME_NAME:\n",
    "        if getattr(row, t_name) != \"\":\n",
    "            time_list.append(f\"{t_name}: {getattr(row, t_name)}\")\n",
    "    time_text = \"None\" if len(time_list) == 0 else \". \".join(time_list)\n",
    "    parsed_data.append([row.AID, row.PID, row.Sentence_id, row.TimeInfo, \"time extraction\", row.Sentence, time_text])\n",
    "    \n",
    "    event_list = []\n",
    "    for e_name in EVENT_NAME:\n",
    "        if e_name == \"DayCare\":\n",
    "            continue\n",
    "        if getattr(row, e_name) > 0:\n",
    "            event_list.append(e_name)\n",
    "    event_text = \"None\" if len(event_list) == 0 else \", \".join(event_list)\n",
    "    parsed_data.append([row.AID, row.PID, row.Sentence_id, row.TimeInfo, \"event detection\", row.Sentence, event_text])\n",
    "\n",
    "parsed_data = pd.DataFrame(\n",
    "    parsed_data, \n",
    "    columns=[\"AID\", \"PID\", \"Sentence_id\", \"TimeInfo\", \"prefix\", \"input_text\", \"target_text\"]\n",
    ")\n",
    "parsed_data.to_excel(\"./data/processed/data_230406_ni.xlsx\", index=False)\n",
    "\n",
    "SEED = 1309\n",
    "TRAIN_RATIO = 0.7\n",
    "EVAL_RATIO = 0.1\n",
    "aid = data.AID.unique()\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(aid)\n",
    "train_aid, eval_aid, test_aid = np.split(aid, [int(TRAIN_RATIO*len(aid)), int((TRAIN_RATIO+EVAL_RATIO)*len(aid))])\n",
    "test_data = parsed_data[parsed_data.AID.isin(test_aid)]\n",
    "test_data.to_excel(\"./data/processed/data_230406_test_ni.xlsx\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use rule to fill in the rows with no TimeInfo\n",
    "* Duration => last time (Duration: 2008-07-23 to 2008-09-12 => 2008-09-12)\n",
    "\n",
    "* Multiple time in the same category => last time (Time_YMD: 2010-06, 2010-06-17 => 2010-06-17)\n",
    "\n",
    "* Multiple time in different categories => all (Time_YMD: 2005-07. Ago_YMD: 4Y. Persistence: 23D => Time_YMD: 2005-07. Ago_YMD: 4Y. Persistence: 23D)\n",
    "\n",
    "* Multiple events => all (Ago_YMD: 4M -> Acute, Episode => Ago_YMD: 4M -> Acute, Ago_YMD: 4M -> Episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0642e57996b514181282526a8dc13c117cd5770cfca79bae0c4a7e20509c8dac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
